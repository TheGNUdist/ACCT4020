<!doctype html>
<html lang="en">
<head>
	<meta charset="utf-8">

	<title>Availability</title>

	<meta name="author" content="Joshua G. Coyne, PhD">
	<link rel="icon" href="../images/memphis.ico" type="image/x-icon" sizes="32x32">

	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

	<meta name="viewport" content="width=device-width, initial-scale=1.01, maximum-scale=1.01, user-scalable=no, minimal-ui">

	<link rel="stylesheet" href="../reveal.js/css/reveal.css">
	<link rel="stylesheet" href="../reveal.js/css/theme/sky.css" id="theme">

	<!-- Personalized CSS -->
	<link rel="stylesheet" href="../css/slides.css">
</head>

<body>
	<div class="reveal">
	<div class="slides">
		<section>
			<h1>Availability</h1>
		</section>

		<section>
			<h2>Accounting Architecture</h2>
			<div class="flex-row">
				<div class="flex-item override"><img src="../images/arch.png" alt="AA Arch" style="max-width: 300px"/></div>
				<div class="flex-item" style="max-width: 65%">
					<p>Availability is the second block of the Control section. The only useful type of information system is one that is available for use. In order to qualify as available for use, a system and its data must be accessible. An accessible system is powered on, connected to the network if necessary, and running all required services. Accessible data is complete an can be read and written by permissioned users.</p>
				</div>
			</div>
		</section>

		<section>
			<h2>Trust Services Criteria</h2>
			<div class="flex-row">
				<div class="flex-item override"><img src="../images/trust-services.png" alt="Trust Services Framework" style="max-width: 300px"/></div>
				<div class="flex-item" style="max-width: 65%">
					<blockquote>The system is available for operation and use as committed or agreed.</blockquote>
					<p>Two threats to availability are data loss and downtime. Three tools combat these threats:</p>
					<ol>
						<li><u><em>Redundancy</em></u> avoids data loss and unplanned downtime.</li>
						<li><u><em>Change management</em></u> limits planned downtime.</li>
						<li><u><em>Interpretation</em></u> preserves readability of legacy data.</li>
					</ol>
				</div>
			</div>
		</section>

		<section>
			<h2>Denied!</h2>
			<p>One major threat to the availability of a cloud system is a <u><em>Denial of Service</em></u> (DoS) attack. This involves flooding a server with data (e.g., ping, email) until the server slows down or crashes. A <u><em>ping</em></u> is a small data packet sent to a server to confirm that it is up and running. In order to generate enough pings or other network traffic to overwhelm a server, these attacks often use a form of distributed computing. A single Command and Control server directs a <u><em>botnet</em></u>, which is a network of zombies, to send traffic to the same server at once. These are <u><em>Distributed Denial of Service</em></u> (DDoS) attacks. Proper use of security tools can protect servers from DoS attacks and clients from becoming unwitting zombies. This is one example of the interplay between security and availability.</p>
		</section>

		<section>
			<h2>Hold Hostage</h2>
			<p>An entire system can be the target of an attack to deny availability. An attack can also be against a single device: server, personal computer, mobile device, IoT device, etc. In many cases, attacks against individual devices have less severe consequences (i.e., lower exposure). Attacks can also be specifically against the data on a system or device, and these targeted attacks can be even more detrimental than rendering a device or cloud service inoperable. One common threat to data availability is <u><em>ransomware</em></u>, which is malware planted by trojan horse or software exploit that locks a device and refuses to release its data until the users pays a ransom. If the users refuses to pay, the ransomware begins to delete the data. Ransomware has successfully crippled entire systems by locking users out of their data.</p>
		</section>

		<section>
			<h2>Foundation of Security</h2>
			<p>Security tools restrict unauthorized access to networks, devices, ports, and data, and frequently unauthorized access is what threatens the availability of a system and its data. Firewalls and other packet filters can prevent pings from crippling a server by forcing the server to ignore them. This precludes the benefit of being able to use a ping to confirm server availability, but all security tools bear a cost of inefficiency. Preventing unpermissioned execution of malware and patching software exploits reduce the likelihood of creating zombies that fail to perform intended functions and instead engaging in nefarious attacks. As a result, security is necessary for ensuring availability. However, additional control activities that focus specific on availability extend the availability benefits of security policies and procedures.</p>
		</section>

		<section>
			<h2>Epic Fail!</h2>
			<p>System components can fail. Hardware can become damaged with age, power surges, or even physical and digital attacks. Networks can lose connectivity because of electrical storms, power outages, or intentional or unintentional severing of cables. Operating systems, file and database systems, and applications can crash. Any one of these can bring down a system and make it unavailable. A <u><em>single point of failure</em></u> (SPOF) is any failure that stops an entire system, and SPOF represents a severe threat to availability. As a result, the main goal of availability-focused internal controls is to limit the number of SPOFs. The primary tool for reducing the number of SPOFs is <u><em>redundancy</em></u>. Redundancy is the duplication of system components so that a system can fail over to an alternative component should one become unavailable.</p>
		</section>

		<section>
			<h2>No Interruptions</h2>
			<p>Because an information system, especially a cloud-based information system, has so many dependent components, complete redundancy is multi-faceted and can take on many different forms. Some redundancy solutions are small and serve a single purpose. An <u><em>uninterruptible power supply</em></u> (UPS) provides power redundancy be supplying electricity in the case of power loss. A UPS is only designed to power devices for a few minutes to provide enough time to shutdown devices or enable a auxiliary power source or backup generator. Although a shutdown device is not available, having the ability to shut them down properly, including unmounting file systems, can preserve availability in the long term by protecting against data corruption and physical wear from sudden power loss.</p>
		</section>

		<section>
			<h2>Deja Vu All Over Again</h2>
			<p>Emergency power, generators, and uninterruptible power supplies create redundancy for an externally provided utility: electricity. Because the entity cannot supply itself with electricity on a consistent basis, this risk to availability is not with the entity's control. Other redundancies also exist to protect against other risks the entity cannot control, such as network outages and natural disasters. ISP or network redundancy provides a server or cloud with multiple Internet service providers in case one becomes unable to provide Internet connectivity. A duplicate site creates geographic redundancy for by duplicating all hardware and software for a server or cloud in a secondary location. Duplicate sites are especially useful in cases of fire and other natural disasters where any local redundancies would also be consumed.</p>
		</section>

		<section>
			<h2>Not My Fault</h2>
			<p>In order for a system to be available to consumers, servers must be powered and connected to the network, ISPs must provide Internet connectivity, and DNS providers must be able to resolve a URL to the server's IP address. This string of requirements creates an almost inevitable risk of SPOF. This was demonstrated when a C&C server took advantage of an exploit in IoT devices to create a botnet that initiated a DDoS attack that brought down the DNS provider Dyn. Dyn was the DNS server for many ISPs in the US. Although the companies affected by the unavailability of Dyn's DNS service (e.g., Netflix and Amazon) also had redundant DNS entries with other DNS servers, ISPs and consumers who accessed Dyn had no mechanism for failing over to an alternative DNS server. Through no fault of their own, these companies' servers were unavailable.</p>
		</section>

		<section>
			<h2>Clusters and Backups</h2>
			<p>In addition to addressing a number risks introduced by relying on vendors to provide power and network connectivity, redundancy also protects against additional risks solely within the entity's control. A distributed computing cluster, in addition to increasing availability by adding additional computing power, allows alternative servers to take over when one server fails. <u><em>Backups</em></u> create duplicates of data on file and database systems in case storage volumes become corrupted or data becomes unintentionally or nefariously deleted, altered, or locked. Regardless of the size or perceived importance of an information system, data backups are a vital control activity. Different backup techniques exist to create data duplicates to satisfy different needs for <u><em>disaster recovery</em></u> (i.e., restoring data and system functionality following a loss of availability).</p>
		</section>

		<section>
			<h2>Back Me Up</h2>
			<p>Two common backup methods are full and incremental. <u><em>Full backups</em></u> create a complete copy of all data during each backup. As a result, each subsequent backup contains the data stored in all earlier backups. Because it is generally good practice to keep more than one backup (e.g., three weeks of daily backups) full backups can quickly become infeasibly large, and backing up a large data volume can take a long time. <u><em>Incremental backups</em></u> only store the changes since the previous backup, also called deltas. Backing up deltas takes less time than backing up an entire volume, but in order to restore an entire incremental backup, it is necessary to restore every delta since the beginning of time. The restoration process can become infeasibly time-consuming.</p>
		</section>

		<section>
			<h2>Easier is not Better</h2>
			<p>One option for reducing the size of full backups or the restoration time of incremental backups is to decrease the frequency of backups (e.g., once a week, instead of once a day). The problem with this option is that the likelihood of data loss will not decrease, but the exposure to a loss will increase, perhaps to an unacceptable level. For example, if the current policy is daily backups and a storage volume fails immediately before that day's backup, at most 24 hours worth of data is irreplaceable. If that policy changes to weekly backups and a storage volumes fails immediately before that week's backup, as much as 168 hours of data is lost! For a large company, that may be hundreds of millions of database records. The solution to more efficient disaster recovery is a riskier backup plan.</p>
		</section>

		<section>
			<h2>Best of Both Worlds</h2>
			<p>One backup solution that reduces the size of backups, as well as the time to create and restore backups, is to combine the concepts of full and incremental backups together by having frequent incremental backups and infrequent full backups. This solution reduces the combined size of all full backups by creating new ones less often without increasing exposure to data loss. It also reduces the time necessary to restore incremental backups by effectively making the beginning of time for restoring deltas much more recent. For example, a backup policy could dictate weekly full backups and daily incremental backups. Relative to daily full backups, this would take one-seventh as much time and space. Relative to incremental backups, it would limit the number of deltas to restore to a maximum of six.</p>
		</section>

		<section>
			<h2>Mirroring</h2>
			<p>A third backup option is a <u><em>mirror</em></u>. As the name implies, a mirror is an exact duplicate of a storage volume. The benefits of a mirror are that the backup procedure usually only involves storing deltas, similar to an incremental backup, and the restoration procedure only involves copying over the current mirror version. This reduces the amount of storage needed for backups, and the amount of time to back up and to restore. The drawback of a mirror is that it is a copy of the current data. If the current data is corrupt, the mirror may also be corrupt. Additionally, if a file is accidentally modified or deleted, the mirror cannot restore an earlier version.</p>
			<p>RAID provides the opportunity to mirror. This is the reason for the name: <em>Redundant</em> Array of Independent Disks.</p>
		</section>

		<section>
			<h2>RAID 1</h2>
			<p>RAID 0 involves combining two physical volumes into one logical volume, called a <em>stripe</em>, so that the file system only sees a single drive with the storage capacity of both physical volumes combined. The purpose of RAID 0 is to maximize the size of a logical volume. Interestingly, one of the problems with RAID 0 is its vulnerability to failure because the stripe becomes corrupted if either physical volume fails. Each drive represents a single point of failure. RAID 1, on the other hand, uses the same two physical volumes as mirrors. The user sees a logical volume with the capacity of one physical volume, and the other physical volume is a duplicate copy (i.e., backup). When redundancy is more important than storage capacity, RAID 1 is more valuable than RAID 0.</p>
		</section>

		<section>
			<h2>Remote Mirror</h2>
			<p>RAID 1 has an additional advantage, namely that the RAID system is designed to repair volumes while the system is live. If one drive becomes corrupted, it is possible to swap out that drive, replace it with a blank one, and have the RAID rebuild the data without ever shutting down the system. This promotes availability by preventing data loss and system downtime simultaneously. Unfortunately, RAID 1 requires that the devices be in the same physical location. If a disaster affects the entire computer, such as fire, electrical surge, or theft, the RAID will no longer provide a backup. Other mirroring tools can create mirrors that are stored off-site and can be accessible in these disaster situations. However, they continue to be subject to the general problems of mirrored backups.</p>
		</section>

		<section>
			<h2>Version Control System</h2>
			<p>One backup tool that combines the best aspects of full, incremental, and mirrored backups is the <u><em>version control system</em></u> (VCS). A VCS backs up deltas, but it can restore all data at once. Additionally, it provides a convenient timeline of changes and can restore the data to any given point in the timeline in history. Version control systems, also called versioning software, are especially prevalent among software developers because in addition to the backup features, they allow for collaboration between multiple contributors. In fact, additional collaboration creates additional backups because each contributor has a complete copy of the data. Developers make edits to their local versions of the data and then push those changes to all contibutors to update their copies.</p>
		</section>

		<section>
			<h2>Redundancy as a Service</h2>
			<p>Several redundancy tools assume that the business entity hosts server and network hardware in a private cloud. However, one of the benefits of public clouds is that the public cloud provider is responsible for redundancy. As a customer of the public cloud provider, the entity has the same claim on availability of the cloud infrastructure as the entity's customers have on the content hosted in that cloud. In order to ensure that the public cloud remains available, the vendor must establish policies for power, network, and geographic redundancy, as well as maintaining a distributed computing environment with redundant server hardware. However, the entity retains the responsibility for data backups and virtual machine or container redundancy to protect against downtime and data loss for reasons unrelated to infrastructure, such as human or software error, data theft, and some types cyber incidents.</p>
		</section>

		<section>
			<h2>Change Management</h2>
			<p>Every information system needs hardware and software updates (i.e., patches) at some point, and these patches sometimes require shutting down or restarting a system. During these periods, any data is unavailable, and any externally facing systems are inaccessible to external stakeholders, such as customers. This can result in a loss of business, missed opportunities for data collection and analysis, or lower customer satisfaction. <u><em>Change management</em></u> is the set of processes that govern system changes and patches. Proper implementation minimizes the negative effects of downtime by (1) scheduling downtime when it would inconvenience the fewest users and (2) structuring patches to reduce the likelihood of an update introducing new errors.</p>
		</section>

		<section>
			<h2>DevOps</h2>
			<p>In order to ensure that system patches will work and not break existing software functionality, developers, who write the code, and IT staff, who maintain the hardware and software and implement the updates, must work together in order to ensure that the system will run smoothly after the update. This is the original premise of DevOps. The goal was to ensure that the computing environment that developers used to write and test the software is as similar as possible to the environment that IT uses to deploy the software. This goal has prompted the proliferation of containers. Since they are much smaller than virtual machines with fewer files, applications, libraries, and settings, it is simple to create an identical environment within a container for development and deployment.</p>
		</section>

		<section>
			<h2>Interpretation</h2>
			<p>One of the original philosophies of the Unix developers was that the basis of all data is plain text. While it is not reasonable to store everything as plain text (e.g., XML, HTML, TXT, CSV, etc.), one overwhelming benefit of plain text is the maximization of data accessibility. Any system can read plain text files, whereas binary files require specific hardware and software to access. However, many software applications, store data in compiled binaries (e.g., DOCX, PDF, PNG, etc.). In order to preserve access to the data stored in binary files, users can either (1) retain in perpetuity the hardware and software necessary to access the data or (2) <u><em>emulate</em></u> the hardware and software when necessary.</p>
		</section>

		<section>
			<h2>Emulation</h2>
			<p>In virtualization, the hypervisor virtualizes computer hardware. In containerization, the container virtualizes the operating system kernel. Emulation is similar to virtualization in that the emulator virtualizes computer hardware. However, in the case of emulation, the hardware is non-existent, or rather the virtualized hardware is different from the system's actual hardware. The emulator translates hardware requests intended for the emulated hardware before communicating with the computer's actual hardware. (This is a popular practice for playing old console and arcade games on personal computers.) Emulated software is similar, in that the software is also non-existent, but the emulator acts like the software. This is useful for replicating the functionality of a single application or an entire operating system.</p>
		</section>

		<section>
			<h2>Big Picture</h2>
			<p>A complete cloud-based information system includes multiple components that promote the availability needs of both internal and external stakeholders by operationalizing the information life cycle or Big Data life cycle. This graphic provides an example of such a system. First, externally facing, department-specific OLTP databases support specialized data collection. Second, one large-scale data store receives a converted copy of the data from each first-stage database and stores it in a centralized location. Third, departments receive curated data back for analysis and reporting.</p>
			<div class="override"><img src="../images/warehouse.png" alt="Data Warehouse"/></div>
		</section>

		<section>
			<h2>Acquisition</h2>
			<p>One of the primary roles of cloud computing for a firm's information system is data acquisition. In many cases, this involves a customer-facing <u><em>online transaction processing</em></u> (OLTP) system. OLTP systems are database systems that support high-frequency online customer transactions, such as sales order creation or banking transactions. The primary purpose of an OLTP system is to ensure that an online system is One definition of OLTP is An OLTP system may be a Web server and database to receive customer orders (e.g., Amazon.com), or it might be a container running a Web app so that banking customers can interact with their accounts. Regardless of the specific configuration, the purpose of OLTP is to acquire data for subsequent analysis.</p>
		</section>

		<section>
			<h2>Integration</h2>
			<p>Because the OLTP systems can have different configurations, the data from each may not be in a uniform format. Before archiving the data, conversion is likely necessary. (This may be in addition to any conversion that is necessary prior to initial entry into the OLTP system.) The staging area provides an intermediate store where data is cleaned and converted. This process is called ETL (Extract, Transform, and Load).</p>
			<div class="override"><img src="../images/warehouse.png" alt="Data Warehouse"/></div>
		</section>

		<section>
			<h2>Clean Up</h2>
			<p>Expertise in data analytics is currently a desirable skill, but one of the precursors to useful data analytics is good data. One of the purposes for teaching the information life cycle is to provide you with the awareness of the need to design systems that will ensure good data for analytics, but for pre-existing systems, sometimes the only solution is to take the data as given. As a result, data specialists confess that a large part of their job is cleaning data (e.g., looking for likely errors, finding duplicate data, matching records in one store with the records in another). It is not fun, but it is necessary.</p>
		</section>

		<section>
			<h2>Data Warehouse and Marts</h2>
			<p>The <u><em>data warehouse</em></u> receives clean data from staging and stores it. <u><em>Data marts</em></u> are small warehouses that store department-specific data for analysis. The warehouse and marts serve two special purposes. First, they allow OLTP systems to remain unencumbered when performing resource-intensive data analysis. Second, they can store pre-calculated OLAP cubes and MapReduce statistics in addition to the original data.</p>
			<div class="override"><img src="../images/warehouse.png" alt="Data Warehouse"/></div>
		</section>

		<section>
			<h2>In Or Out?</h2>
			<p>With an increase in desire and capacity to perform real-time data analytics, this traditional four-step data warehouse model has fallen out of favor. Some (e.g., representatives from EY Tax) have even asserted that the data warehouse was a thing of the past, but this is an overstatement. Newer tools allow for real-time analysis of data on OLTP systems, and greater processing power has made this a possibility without overloading the OLTP system and making it unavailable for data acquisition. However, these tools have not completely replaced the need for data warehouses when analysis requires data from multiple OLTP systems; when the analysis requires resource-intensive pre-calculation, such as MOLAP or MapReduce; or when the answers sought are not needed in real time.</p>
		</section>

		<section>
			<h2>Accounting Tie-in</h2>
			<p>Availability addresses access to data and network systems. Accounting regulation has strict rules about the preservation of accounting data. For example, tax authorities can audit the past three years' returns; financial statements require presentation of prior years' numbers for comparability; restatements or the adoption of new standards may require retroactive adjustments. Old accounting data must be available. Furthermore, much of accounting work is cloud-based. For example, data may be stored on a shared file server. In addition to these, internal decision makers require constant access to data, and systems must be available to collect and analyze it. That is the requirement of the Information Era.</p>
		</section>
	</div>
	</div>

	<script src="../reveal.js/lib/js/head.min.js"></script>
	<script src="../reveal.js/js/reveal.js"></script>
	<script src="../js/slides.js"></script>
</body>
</html>
